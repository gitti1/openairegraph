---
title: "Accessing and analysing the OpenAIRE Research Graph data dumps"
bibliography: literature.bib
csl: springer-lecture-notes-in-computer-science.csl
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    template: lncs.sty
link-citations: yes
pkgdown:
  as_is: true
vignette: >
  %\VignetteIndexEntry{foo}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE
)
```

# Introduction

OpenAIRE has collected and interlinked scholarly data from various openly available sources for over ten years. In December 2019, this European open science network released the OpenAIRE Research Graph Dump [@manghi_paolo_2019_3516918], a big scholarly dataset that comprises metadata about more than 100 million research publications and 18 million research datasets, as well as the relationships between them. These metadata are furthermore connected to open access locations and disambiguated information about persons, organisations and funders. 

Like most big scholarly data dumps, the OpenAIRE Research Graph offers many data analytics opportunities, but working with it can be challenging for data science practitioners. One reason is the sheer size of the data release. Although the OpenAIRE Research Graph Dump is already split into several files, most of these data files are too large to fit the memory of a moderately equipped personal computer, when directly imported into statistical computing environments like R. Another challenge is the format. Statistical methods require tidy data, i.e. datasets with a rectangular structure made up of unambiguous columns and rows [@Wickham_2014]. The dump, however, consists of compressed XML-files following the comprehensive OpenAIRE data model [@manghi_paolo_2019_2643199], from which only certain elements may be needed for a specific data analysis.

In this paper, we will introduce the R package `openairegraph` that helps to transform the large OpenAIRE Research Graph dumps into relevant small datasets for analytical purposes. It aims at data science practitioners and researchers alike who wish to conduct their own statistical analysis using the OpenAIRE Research Graph, but are wary of handling its large data dumps.  Our implementation does not only consist of tools that allow to explore and parse specific information from the OpenAIRE Research Graph dump. The R package structure also provides a standard way to ensure the computational reproducibility of these tools. To demonstrate the usefulness of our implementation, we will benchmark the open access activities of grant-supported projects affiliated with the University of Göttingen against the overall uptake across the funding activities in the European HORIZON 2020 funding programme.

# Background

- R tooling for data analytics workflow
- review of related appproaches, 
  - general,e.g. research compendium
  - more domain specific, e.g. ropensci packages and dev guide, bibliometrix

# Implementation

The R package `openairegraph`, which is available on GitHub as a development version [^1], has two sets of functions. The first set provides helpers to split a large OpenAIRE Research Graph data dump into separate, de-coded XML records that can be stored individually. The other set consists of parsers that convert data from these XML files to a table-like representation following the tidyverse philosophy, a popular approach to practise data science [@tidyverse]. Splitting, de-coding and parsing are essential steps before analysing the OpenAIRE Research Graph.

The function `openairegraph::oarg_decode()` splits and de-codes each record. Storing the records individually will allow to process the files independent from each other, which is a common approach when working with big data. 

```r
library(openairegraph)
library(jsonlite) # tools to work with json files
library(tidyverse) # tools from the tidyverse useful for data analysis
# download the file from Zenodo and store it locally
oaire <- jsonlite::stream_in(file("data/h2020_results.gz"))
openairegraph::oarg_decode(oaire, records_path = "data/records/", 
  limit = 500, verbose = FALSE)
```

Because the dumps are quite large, the function furthermore has a parameter that allows setting a limit, which is helpful for inspecting the output first. By default, a progress bar presents the current state of the process.

So far, there are four parsers available to consume the H2020 results set:

- `openairegraph::oarg_publications_md()` retrieves basic publication metadata complemented by author details and access status
- `openairegraph::oarg_linked_projects()` parses grants linked to publications
- `openairegraph::oarg_linked_ftxt()` gives full-text links including access information
- `openairegraph::oarg_linked_affiliations()` parses affiliation data

These parsers can be used alone, or together like this:

```r
library(xml2) # working with xml files
library(future) # parallel computing
library(future.apply) # functional programming in parallel 
openaire_records <- list.files("data/records", full.names = TRUE)
future::plan(multisession) # initalize parallel computing
oaire_data <- future.apply::future_lapply(openaire_records,
 function(files) {
  # load xml file
  doc <- xml2::read_xml(files)
  # parser
  out <- oarg_publications_md(doc)
  out$linked_projects <- list(oarg_linked_projects(doc))
  out$linked_ftxt <- list(oarg_linked_ftxt(doc))
  # use file path as id
  out$id <- files
  out
})
oaire_df <- dplyr::bind_rows(oaire_data)
```

First, we obtained the locations of the de-coded XML records. After that, we read each XML file using the `xml2` [@xml2] package, and applied three parsers: `openairegraph::oarg_publications_md()`, `openairegraph::oarg_linked_projects()` and `openairegraph::oarg_linked_ftxt()`.  We used the `future` [@future] and `future.apply` [@future_apply] packages to enable reading and parsing these records simultaneously with multiple R sessions. Running code in parallel reduces the execution time. Finally, we put together the individually created datasets into one dataset.

Documentation is an essential part of the R package workflow. For each parser, we documented the usage, as well as the returned output. The latter is represented as data coding tables with an description of each variables, making it easier to grasp the otherwise complex data structure of the OpenAIRE Research Graph. While executable examples are provided for each function, an introductory vignette, a long-form documentation, demonstrates a workflow using the package. All documentation is automatically rendered to an R manual, and a package website was built with `pkgdown` [@pkgdown].

# Use-Case

A main purpose of the OpenAIRE Research Graph, which links grants to publications and open access full-texts, is monitoring the efficacy of open access mandates. Here, we will demonstrate how to benchmark the compliance with the open access mandate of the European Commission’s HORIZON 2020 funding programme (H2020) against related funding activities using OpenAIRE data and our openairegraph package. We will focus on projects affiliated with the University of Göttingen. In doing so, we want to take into account that open access mandates generally lead to above-average open access uptake levels. Yet, they can vary by discipline and funding activity [@Larivi_re_2018].

```{r}
library(openairegraph)
library(jsonlite)
library(tidyverse)
library(scales)
library(here)
oaire_df <- jsonlite::stream_in(file(here("data", "h2020_parsed.json")), verbose = FALSE) %>%
  tibble::as_tibble()

pubs_projects <- oaire_df %>%
  select(id, type, best_access_right, linked_projects) %>%
  unnest(linked_projects) 

oa_monitor_ec <- pubs_projects %>%
  filter(funding_level_0 == "H2020") %>%
  mutate(funding_scheme = fct_infreq(funding_level_1)) %>%
  group_by(funding_scheme,
           project_code,
           project_acronym,
           best_access_right) %>%
  summarise(oa_n = n_distinct(id)) %>% # per pub
  mutate(oa_prop = oa_n / sum(oa_n)) %>%
  filter(best_access_right == "Open Access") %>%
  ungroup() %>%
  mutate(all_pub = as.integer(oa_n / oa_prop)) 
  ```

As a start, we imported and transformed the whole h2020_results.gz dump following the above-described methods. The resulting dataset comprises 84,781 literature publications from 9,008 H2020 projects. After that, we identified H2020 projects with participation from the University of Göttingen using data from CORDIS, the European Commission’s research information portal. We calculated the open access share per project affiliated with the university and the open access shares for each project in the corresponding H2020 activity like European Research Council grants or Marie Skłodowska-Curie activities.


```{r,  fig.cap = "Open Access Compliance Rates of Horizon 2020 projects affiliated with the University of Göttingen (purple dots) relative to the overall performance of the coressponding funding activities, visualised as a box plot. Only projects with at least five publications were considered. Data: OpenAIRE Research Graph[@manghi_paolo_2019_3516918]"}
# load local copy downloaded from the EC open data portal
cordis_org <-
  readr::read_delim(
    here("data", "cordis-h2020organizations.csv"),
    delim = ";",
    locale = locale(decimal_mark = ",")
  ) %>%
  # data cleaning
  mutate_if(is.double, as.character)
# tag projects affiliated with the University of Göttingen
ugoe_projects <- cordis_org %>%
  filter(shortName %in% c("UGOE", "UMG-GOE")) %>% 
  select(project_id = projectID, role, project_acronym = projectAcronym)

pubs_projects_ugoe <- pubs_projects %>%
  mutate(ugoe_project = funding_level_0 == "H2020" & project_code %in% ugoe_projects$project_id)

ugoe_funding_programme <- pubs_projects_ugoe %>% 
  filter(ugoe_project == TRUE) %>%
  group_by(funding_level_1, project_code) %>% 
  # min 5 pubs
  summarise(n = n_distinct(id)) %>%
  filter(n >= 5) %>%
  distinct(funding_level_1, project_code)
goe_oa <- oa_monitor_ec %>%
  # min 5 pubs
  filter(all_pub >=5) %>%
  filter(funding_scheme %in% ugoe_funding_programme$funding_level_1) %>%
  mutate(ugoe = project_code %in% ugoe_funding_programme$project_code) %>%
  mutate(`H2020 project` = paste0(project_acronym, " | OA share: ", round(oa_prop * 100, 0), "%"))
# plot as interactive graph using plotly
ggplot(goe_oa, aes(funding_scheme, oa_prop)) +
  geom_boxplot() +
  geom_jitter(data = filter(goe_oa, ugoe == TRUE),
               aes(label = `H2020 project`),
             colour = "#AF42AE",
             alpha = 0.9,
             size = 3,
             width = 0.25) +
  geom_hline(aes(
    yintercept = mean(oa_prop),
    color = paste0("Mean=", as.character(round(
      mean(oa_prop) * 100, 0
    )), "%")
  ),
  linetype = "dashed",
  size = 1) +
  geom_hline(aes(
    yintercept = median(oa_prop),
    color = paste0("Median=", as.character(round(
      median(oa_prop) * 100, 0
    )), "%")
  ),
  linetype = "dashed",
  size = 1) +
  scale_color_manual(NULL, values = c("orange", "darkred")) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 5L)) +
  labs(x = NULL,
       y = "Open Access Percentage",
       caption = "Data: OpenAIRE Research Graph") +
  theme_minimal() +
  theme(legend.position = "top",
        legend.justification = "right")

```

Figure 3 shows that many H2020-projects with University of Göttingen participation have an uptake of open access to grant-supported publications that is above the average in the peer group. At the same time, some perform below expectation. Together, this provides a valuable insight into open access compliance at the university-level, especially for research support staff who are in charge of helping grantees to make their work open access. They can, for instance, point grantees to OpenAIRE-compliant repositoires for self-archiving their works. 

# Discussion and Conclusion

# References

[^1]: <https://github.com/subugoe/openairegraph>